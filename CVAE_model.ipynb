{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfESBIaGfgqB"
      },
      "source": [
        "# Implementation of Convolutional Variational AutoEncoder (CVAE)\n",
        "    \n",
        "     Author: Luu-Nguyen Van\n",
        "     \n",
        "     Email:  luu@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k73RRiAIksP6"
      },
      "source": [
        "# Prequiste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KPyMpYFdfgqE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.distributions.normal import Normal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqpiaztmq5bi"
      },
      "source": [
        "# Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DXvnf7WJfgqF"
      },
      "outputs": [],
      "source": [
        "dataset_path = '~/datasets'\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "LR = 1e-3\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 2\n",
        "EPOCHS = 100\n",
        "SHAPE_BEFORE_FLATTENING = (128, IMAGE_SIZE // 8, IMAGE_SIZE // 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjusUs9TfgqF"
      },
      "source": [
        "###    Step 1. Load (or download) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L5jBuApQfgqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba1cefd-6103-4dd5-c141-6b3f7f858991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /root/datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 128MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/datasets/MNIST/raw/train-images-idx3-ubyte.gz to /root/datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 33.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/datasets/MNIST/raw/train-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /root/datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 45.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.64MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "mnist_transform = transforms.Compose([\n",
        "        torchvision.transforms.Resize(32), transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "\n",
        "train_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n",
        "test_dataset  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "test_loader  = DataLoader(dataset=test_dataset,  batch_size=BATCH_SIZE, shuffle=True, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUNnm0CJfgqG"
      },
      "source": [
        "### Step 2. Define model: Convolutional Variational AutoEncoder (CVAE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i90zk_lnhtVi"
      },
      "source": [
        "**2.1. Define a class for sampling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LM1t7PMFhsi5"
      },
      "outputs": [],
      "source": [
        "# This class will be used in the encoder for sampling in the latent space\n",
        "class Sampling(nn.Module):\n",
        "    def forward(self, z_mean, z_log_var):\n",
        "        # get the shape of the tensor for the mean and log variance\n",
        "        batch, dim = z_mean.shape\n",
        "        # generate a normal random tensor (epsilon) with the same shape as z_mean\n",
        "        # this tensor will be used for reparameterization trick\n",
        "        epsilon = Normal(0, 1).sample((batch, dim)).to(z_mean.device)\n",
        "        # apply the reparameterization trick to generate the samples in the\n",
        "        # latent space\n",
        "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZMH53_iCSF"
      },
      "source": [
        "**2.2. Implement Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "e1kl5Q6BfgqG"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, image_size, embedding_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        # define the convolutional layers for downsampling and feature extraction\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
        "        # define a flatten layer to flatten the tensor before feeding it into the fully connected layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        # define fully connected layers to transform the tensor into the desired embedding dimensions\n",
        "        self.fc_mean = nn.Linear(128 * (image_size // 8) * (image_size // 8), embedding_dim)\n",
        "        self.fc_log_var = nn.Linear(128 * (image_size // 8) * (image_size // 8), embedding_dim)\n",
        "        # initialize the sampling layer\n",
        "        self.sampling = Sampling()\n",
        "    def forward(self, x):\n",
        "        # apply convolutional layers with relu activation function\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        # flatten the tensor\n",
        "        x = self.flatten(x)\n",
        "        # get the mean and log variance of the latent space distribution\n",
        "        z_mean = self.fc_mean(x)\n",
        "        z_log_var = self.fc_log_var(x)\n",
        "        # sample a latent vector using the reparameterization trick\n",
        "        z = self.sampling(z_mean, z_log_var)\n",
        "        return z_mean, z_log_var, z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a6fguSkiQHv"
      },
      "source": [
        "**2.3. Implement Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oyVb4qZjfgqH"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, shape_before_flattening):\n",
        "        super(Decoder, self).__init__()\n",
        "        # define a fully connected layer to transform the latent vector back to the shape before flattening\n",
        "        self.fc = nn.Linear(embedding_dim, shape_before_flattening[0] * shape_before_flattening[1] * shape_before_flattening[2])\n",
        "        # define a reshape function to reshape the tensor back to its original shape\n",
        "        self.reshape = lambda x: x.view(-1, *shape_before_flattening)\n",
        "        # define the transposed convolutional layers for the decoder to upsample and generate the reconstructed image\n",
        "        self.deconv1 = nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1)\n",
        "        self.deconv3 = nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1)\n",
        "    def forward(self, x):\n",
        "        # pass the latent vector through the fully connected layer\n",
        "        x = self.fc(x)\n",
        "        # reshape the tensor\n",
        "        x = self.reshape(x)\n",
        "        # apply transposed convolutional layers with relu activation function\n",
        "        x = F.relu(self.deconv1(x))\n",
        "        x = F.relu(self.deconv2(x))\n",
        "        # apply the final transposed convolutional layer with a sigmoid activation to generate the final output\n",
        "        x = torch.sigmoid(self.deconv3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W01ZGdQOicls"
      },
      "source": [
        "**2.3. Implement model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8HS4LlVpfgqH"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Model, self).__init__()\n",
        "        # initialize the encoder and decoder\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    def forward(self, x):\n",
        "        # pass the input through the encoder to get the latent vector\n",
        "        z_mean, z_log_var, z = self.encoder(x)\n",
        "        # pass the latent vector through the decoder to get the reconstructed image\n",
        "        reconstruction = self.decoder(z)\n",
        "        # return the mean, log variance and the reconstructed image\n",
        "        return z_mean, z_log_var, reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnneXLfhfgqI",
        "outputId": "65fbd130-d132-4dec-a185-280af822dbe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (encoder): Encoder(\n",
            "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "    (fc_mean): Linear(in_features=2048, out_features=2, bias=True)\n",
            "    (fc_log_var): Linear(in_features=2048, out_features=2, bias=True)\n",
            "    (sampling): Sampling()\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (fc): Linear(in_features=2, out_features=2048, bias=True)\n",
            "    (deconv1): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (deconv2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (deconv3): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 16, 16]             320\n",
            "            Conv2d-2             [-1, 64, 8, 8]          18,496\n",
            "            Conv2d-3            [-1, 128, 4, 4]          73,856\n",
            "           Flatten-4                 [-1, 2048]               0\n",
            "            Linear-5                    [-1, 2]           4,098\n",
            "            Linear-6                    [-1, 2]           4,098\n",
            "          Sampling-7                    [-1, 2]               0\n",
            "           Encoder-8  [[-1, 2], [-1, 2], [-1, 2]]               0\n",
            "            Linear-9                 [-1, 2048]           6,144\n",
            "  ConvTranspose2d-10             [-1, 64, 8, 8]          73,792\n",
            "  ConvTranspose2d-11           [-1, 32, 16, 16]          18,464\n",
            "  ConvTranspose2d-12            [-1, 1, 32, 32]             289\n",
            "          Decoder-13            [-1, 1, 32, 32]               0\n",
            "================================================================\n",
            "Total params: 199,557\n",
            "Trainable params: 199,557\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.25\n",
            "Params size (MB): 0.76\n",
            "Estimated Total Size (MB): 1.02\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the encoder and decoder models\n",
        "encoder = Encoder(IMAGE_SIZE, EMBEDDING_DIM)\n",
        "decoder = Decoder(EMBEDDING_DIM, SHAPE_BEFORE_FLATTENING)\n",
        "\n",
        "# Pass the encoder and decoder to VAE class\n",
        "model = Model(encoder=encoder, decoder=decoder).to(DEVICE)\n",
        "print(model)\n",
        "from torchsummary import summary\n",
        "summary(model, (1, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTpfk2K6fgqI"
      },
      "source": [
        "### Step 3. Define Loss function (reprod. loss) and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JfMP-YSWfgqI"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "BCE_loss = nn.BCELoss()\n",
        "\n",
        "def loss_function(x, x_hat, mean, log_var):\n",
        "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "    KLD      = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
        "\n",
        "    return reproduction_loss + KLD\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTJf9iNufgqJ"
      },
      "source": [
        "### Step 4. Train Convolutional Variational AutoEncoder (CVAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "JOCa0bIkfgqJ",
        "scrolled": false,
        "outputId": "9ba2392e-880c-4459-9da4-2695830e43d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training VAE...\n",
            "\tEpoch 1 complete! \tAverage Loss:  248.51301317408348\n",
            "\tEpoch 2 complete! \tAverage Loss:  218.10322022819722\n",
            "\tEpoch 3 complete! \tAverage Loss:  212.6944215463091\n",
            "\tEpoch 4 complete! \tAverage Loss:  210.05263604781925\n",
            "\tEpoch 5 complete! \tAverage Loss:  208.25001946024858\n",
            "\tEpoch 6 complete! \tAverage Loss:  206.87699919449227\n",
            "\tEpoch 7 complete! \tAverage Loss:  205.85392396452588\n",
            "\tEpoch 8 complete! \tAverage Loss:  205.01914326638268\n",
            "\tEpoch 9 complete! \tAverage Loss:  204.385221980043\n",
            "\tEpoch 10 complete! \tAverage Loss:  203.75346077315453\n",
            "\tEpoch 11 complete! \tAverage Loss:  203.20021794012797\n",
            "\tEpoch 12 complete! \tAverage Loss:  202.7216438203891\n",
            "\tEpoch 13 complete! \tAverage Loss:  202.32507131244736\n",
            "\tEpoch 14 complete! \tAverage Loss:  201.9035426153063\n",
            "\tEpoch 15 complete! \tAverage Loss:  201.57391579708397\n",
            "\tEpoch 16 complete! \tAverage Loss:  201.1577654485387\n",
            "\tEpoch 17 complete! \tAverage Loss:  200.84707200285465\n",
            "\tEpoch 18 complete! \tAverage Loss:  200.5908624980849\n",
            "\tEpoch 19 complete! \tAverage Loss:  200.40405915055737\n",
            "\tEpoch 20 complete! \tAverage Loss:  200.02977984742904\n",
            "\tEpoch 21 complete! \tAverage Loss:  199.86637058481972\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bc6313b05ebe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_weights.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"Start training VAE...\")\n",
        "# initialize the best validation loss as infinity\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    overall_loss = 0\n",
        "    for batch_idx, (x, _) in enumerate(train_loader):\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        mean, log_var, x_hat = model(x)\n",
        "        loss = loss_function(x, x_hat, mean, log_var)\n",
        "\n",
        "        overall_loss += loss.item()\n",
        "        # save best vae model weights based on validation loss\n",
        "        if overall_loss < best_loss:\n",
        "            best_loss = overall_loss\n",
        "            torch.save(model.state_dict(), 'model_weights.pt')\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / (batch_idx*BATCH_SIZE))\n",
        "\n",
        "print(\"Finish!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIDi-1zUfgqK"
      },
      "source": [
        "### Step 5. Generate images from test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hxv3euupfgqK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXZNyB0zfgqK",
        "outputId": "86692ee9-c7c5-4bde-b398-3149744adae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/157 [00:00<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (x, _) in enumerate(tqdm(test_loader)):\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        _, _, x_hat = model(x)\n",
        "\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_LrH_BRlfgqK"
      },
      "outputs": [],
      "source": [
        "def show_image(x, idx):\n",
        "    x = x.view(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(x[idx].cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaSdjaY0o9EX"
      },
      "source": [
        "**Original image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Plbe4qWCfgqK",
        "scrolled": true,
        "outputId": "9d3b6d98-5cf4-4107-bc99-10b300e499a9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_image(x, idx=11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM2niJ5spFIo"
      },
      "source": [
        "**Reconstruction image after going through the network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "w6pVZofPfgqK",
        "outputId": "d18f4232-845e-4238-e41d-255dabb82086"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_image(x_hat, idx=11)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print Weights and Output of each Layers information"
      ],
      "metadata": {
        "id": "YYAAx_bYXtPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the weights from the specified file\n",
        "model.load_state_dict(torch.load(\"/content/model_weights.pt\"))\n",
        "print(\"WEIGHTS:\")\n",
        "# Print weight information for each layer\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()}\")\n",
        "    weights = param.data\n",
        "    min_weight = torch.min(weights)\n",
        "    max_weight = torch.max(weights)\n",
        "    print(f\"Range - Min: {min_weight.item()}, Max: {max_weight.item()} \\n\")\n",
        "\n",
        "# Initialize variables to store the overall minimum and maximum values for each layer\n",
        "min_values = {}\n",
        "max_values = {}\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "def hook_fn(module, input, output):\n",
        "    layer_name = module.__class__.__name__\n",
        "    min_val = torch.min(output).item()\n",
        "    max_val = torch.max(output).item()\n",
        "\n",
        "    if layer_name not in min_values:\n",
        "        min_values[layer_name] = min_val\n",
        "        max_values[layer_name] = max_val\n",
        "    else:\n",
        "        min_values[layer_name] = min(min_values[layer_name], min_val)\n",
        "        max_values[layer_name] = max(max_values[layer_name], max_val)\n",
        "\n",
        "# Register forward hooks for Linear and Conv layers\n",
        "for module in model.modules():\n",
        "    if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        module.register_forward_hook(hook_fn)\n",
        "\n",
        "# Iterate over the test loader to trigger the hooks\n",
        "for image, _ in test_loader:\n",
        "    image = image.to(device)\n",
        "    with torch.no_grad():\n",
        "        _ = model(image)  # Perform a forward pass\n",
        "\n",
        "print(\"\\nOUTPUT:\")\n",
        "# Print the minimum and maximum values for each layer\n",
        "for layer_name in min_values:\n",
        "    print(f\"Layer: {layer_name} | Min: {min_values[layer_name]}, Max: {max_values[layer_name]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9J56r1lXsfq",
        "outputId": "956bac2e-39d5-4c56-d6e0-363e9949d4b6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WEIGHTS:\n",
            "Layer: encoder.conv1.weight | Size: torch.Size([32, 1, 3, 3])\n",
            "Range - Min: -1.9035922288894653, Max: 0.5418812036514282 \n",
            "\n",
            "Layer: encoder.conv1.bias | Size: torch.Size([32])\n",
            "Range - Min: -0.333332359790802, Max: 0.16348746418952942 \n",
            "\n",
            "Layer: encoder.conv2.weight | Size: torch.Size([64, 32, 3, 3])\n",
            "Range - Min: -0.8140051364898682, Max: 0.8200294375419617 \n",
            "\n",
            "Layer: encoder.conv2.bias | Size: torch.Size([64])\n",
            "Range - Min: -0.0926290825009346, Max: 0.11904893070459366 \n",
            "\n",
            "Layer: encoder.conv3.weight | Size: torch.Size([128, 64, 3, 3])\n",
            "Range - Min: -1.2458655834197998, Max: 0.4167528748512268 \n",
            "\n",
            "Layer: encoder.conv3.bias | Size: torch.Size([128])\n",
            "Range - Min: -0.08262688666582108, Max: 0.20946218073368073 \n",
            "\n",
            "Layer: encoder.fc_mean.weight | Size: torch.Size([2, 2048])\n",
            "Range - Min: -0.6522362232208252, Max: 0.721634566783905 \n",
            "\n",
            "Layer: encoder.fc_mean.bias | Size: torch.Size([2])\n",
            "Range - Min: -0.03896128013730049, Max: 0.0006666849949397147 \n",
            "\n",
            "Layer: encoder.fc_log_var.weight | Size: torch.Size([2, 2048])\n",
            "Range - Min: -0.8084071278572083, Max: 0.5381798148155212 \n",
            "\n",
            "Layer: encoder.fc_log_var.bias | Size: torch.Size([2])\n",
            "Range - Min: -0.4906708598136902, Max: -0.45828118920326233 \n",
            "\n",
            "Layer: decoder.fc.weight | Size: torch.Size([2048, 2])\n",
            "Range - Min: -1.1325182914733887, Max: 1.0149246454238892 \n",
            "\n",
            "Layer: decoder.fc.bias | Size: torch.Size([2048])\n",
            "Range - Min: -1.0058456659317017, Max: 1.249294400215149 \n",
            "\n",
            "Layer: decoder.deconv1.weight | Size: torch.Size([128, 64, 3, 3])\n",
            "Range - Min: -0.7400577664375305, Max: 0.603794515132904 \n",
            "\n",
            "Layer: decoder.deconv1.bias | Size: torch.Size([64])\n",
            "Range - Min: -0.19019635021686554, Max: 0.12670083343982697 \n",
            "\n",
            "Layer: decoder.deconv2.weight | Size: torch.Size([64, 32, 3, 3])\n",
            "Range - Min: -1.2814171314239502, Max: 0.5635204315185547 \n",
            "\n",
            "Layer: decoder.deconv2.bias | Size: torch.Size([32])\n",
            "Range - Min: -0.20253998041152954, Max: 0.37236350774765015 \n",
            "\n",
            "Layer: decoder.deconv3.weight | Size: torch.Size([32, 1, 3, 3])\n",
            "Range - Min: -0.7214884757995605, Max: 0.8457860946655273 \n",
            "\n",
            "Layer: decoder.deconv3.bias | Size: torch.Size([1])\n",
            "Range - Min: 0.14455127716064453, Max: 0.14455127716064453 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-c5ac7153f574>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/model_weights.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "OUTPUT:\n",
            "Layer: Conv2d | Min: -7.178086757659912, Max: 2.6956684589385986\n",
            "Layer: Linear | Min: -9.192024230957031, Max: 4.810401439666748\n",
            "Layer: ConvTranspose2d | Min: -113.92424774169922, Max: 70.95729064941406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8ReLyGPfgqL"
      },
      "source": [
        "### Step 6. Generate image from noise vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rPaMsBhfgqL"
      },
      "source": [
        "**Please note that this is not the correct generative process.**\n",
        "\n",
        "* Even if we don't know exact p(z|x), we can generate images from noise, since the loss function of training CVAE regulates the q(z|x) (simple and tractable posteriors) must close enough to N(0, I). If q(z|x) is close to N(0, I) \"enough\"(but not tightly close due to posterior collapse problem), N(0, I) may replace the encoder of CVAE.\n",
        "\n",
        "* To show this, I just tested with a noise vector sampled from N(0, I) similar with Generative Adversarial Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMRrCgQZfgqL",
        "outputId": "c12758dc-2bc6-46d3-952e-73aca5fda151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 2])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    noise = torch.randn(BATCH_SIZE, EMBEDDING_DIM).to(DEVICE)\n",
        "    print(noise.shape)\n",
        "    generated_images = decoder(noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Xa-TAe2PfgqL"
      },
      "outputs": [],
      "source": [
        "save_image(generated_images.view(BATCH_SIZE, 1, IMAGE_SIZE, IMAGE_SIZE), 'generated_sample.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4Pv4gLdfgqL"
      },
      "outputs": [],
      "source": [
        "show_image(generated_images, idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry4uYemWfgqL"
      },
      "outputs": [],
      "source": [
        "show_image(generated_images, idx=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsMQ08o9fgqL"
      },
      "outputs": [],
      "source": [
        "show_image(generated_images, idx=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFq1TFu8fgqL"
      },
      "outputs": [],
      "source": [
        "show_image(generated_images, idx=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRB_u-MyfgqM"
      },
      "outputs": [],
      "source": [
        "show_image(generated_images, idx=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbmJRCx5fgqM"
      },
      "outputs": [],
      "source": [
        "show_image(generated_images, idx=62)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}